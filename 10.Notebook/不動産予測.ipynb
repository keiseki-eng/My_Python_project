{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX0ZdLopCj-f"
      },
      "source": [
        "★実行環境の選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mwpl2bQsv9Gp"
      },
      "outputs": [],
      "source": [
        "# VSCodeの場合\n",
        "# edi_flg = 1\n",
        "# Googlre Colabの場合\n",
        "edi_flg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drmdF2ct3EBt",
        "outputId": "45810595-f514-4c76-8e53-54c36ee36979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/My_Python_project\n",
            "fatal: destination path 'My_Python_project' already exists and is not an empty directory.\n",
            "From https://github.com/keiseki-eng/My_Python_project\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "if edi_flg == 0:\n",
        "  # GoogleColabにGitHubリポジトリをクローンする用\n",
        "  %cd /content/My_Python_project\n",
        "\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  !git clone https://github.com/keiseki-eng/My_Python_project\n",
        "  !git pull origin main\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zq_NLJ4i2H_f"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A8CzKrBbb7O",
        "outputId": "34456b3a-c635-42fc-df8d-416bb906b5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: japanize-matplotlib in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from japanize-matplotlib) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "if edi_flg == 0:\n",
        "    !pip install japanize-matplotlib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "import japanize_matplotlib #日本語表示対応\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Notebook から src ディレクトリを追加\n",
        "# sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
        "sys.path.append(\"/home/keiseki/My_Python_project/src\")\n",
        "\n",
        "# これで src/preprocess/make_tag_features.py が import 可能\n",
        "# from preprocess.make_tag_features import create_tag_features, extract_unique_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31-po6qhAZLi",
        "outputId": "cce6f5f4-5ca8-43cd-df3a-06effac74255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if edi_flg == 0:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "csR9wtPUwBcd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=pd.errors.PerformanceWarning\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vel6-57c2H_i"
      },
      "source": [
        "## 01.config読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jOVigtnz2H_j"
      },
      "outputs": [],
      "source": [
        "# VSCode用\n",
        "if edi_flg==1:\n",
        "  conf_path = os.path.join( '../config/config.yaml')\n",
        "  with open(conf_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# GoogleColab用\n",
        "elif edi_flg==0:\n",
        "  conf_path = \"My_Python_project/config/config.yaml\"\n",
        "  with open(conf_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ugpvxdsV2H_k"
      },
      "outputs": [],
      "source": [
        "# 定義した特徴量リストを読み込み\n",
        "feature_list = config['FEATURE']['FEATURE_LIST']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kH-U-6i52H_k"
      },
      "outputs": [],
      "source": [
        "# カテゴリカルカラムのリストを定義\n",
        "categorical_cols = config['FEATURE']['CATEGORICAL_COLS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDwS_bGw2H_k"
      },
      "source": [
        "## 02.データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLIBSLbkZpre",
        "outputId": "104bb870-87e1-4da0-adba-8521f1aae89f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# VSCode用\n",
        "if edi_flg==1:\n",
        "# ドライブ内のファイルパスを指定\n",
        "    train_path = '../20.Data/processed_train.pkl'\n",
        "    df_train = pd.read_pickle(train_path)\n",
        "\n",
        "# GoogleColab用\n",
        "elif edi_flg==0:\n",
        "    # GoogleDriveをマウントしてファイル読み込み準備\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # ドライブ内のファイルパスを指定\n",
        "    train_path = '/content/drive/MyDrive/Colab Notebooks/不動産予測/processed_train.pkl'\n",
        "    df_train = pd.read_pickle(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KfjIljDC2H_l"
      },
      "outputs": [],
      "source": [
        "# testデータの読み込み\n",
        "# VSCode用\n",
        "if edi_flg==1:\n",
        "    test_path = '../20.Data/processed_test.pkl'\n",
        "    df_test = pd.read_pickle(test_path)\n",
        "\n",
        "# GoogleColab用\n",
        "elif edi_flg==0:\n",
        "    # ドライブ内のファイルパスを指定\n",
        "    test_path = '/content/drive/MyDrive/Colab Notebooks/不動産予測/processed_test.pkl'\n",
        "    df_test = pd.read_pickle(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5qr2Z4KqyG-"
      },
      "source": [
        "## ★うまく行けば特徴量作成ファイル、configに反映！価格帯のような特徴量を作成"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LOG = 15  # expm1(15) ≒ 3,269,000\n",
        "\n",
        "def safe_expm1(x):\n",
        "    return np.expm1(np.clip(x, -10, MAX_LOG))\n"
      ],
      "metadata": {
        "id": "DZ2Pfnvr2-hV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cuEFTkHrq3eQ"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 擬似 price_band 作成（リークなし）\n",
        "# ===============================\n",
        "\n",
        "def make_pseudo_price(df):\n",
        "    score = (\n",
        "        # 面積（最重要）\n",
        "        0.55 * np.log1p(df[\"unit_area\"])\n",
        "\n",
        "        # 駅距離（近いほど高い）\n",
        "        - 0.30 * np.log1p(df[\"walk_distance1\"] + 1)\n",
        "\n",
        "        # 築年数（新しいほど高い）\n",
        "        - 0.20 * np.log1p(df[\"building_age\"] + 1)\n",
        "\n",
        "        # 間取り数（部屋数が多いほど高い）\n",
        "        + 0.15 * np.log1p(df[\"room_count\"] + 1)\n",
        "    )\n",
        "\n",
        "    # walk_distance2 が存在すれば軽く効かせる\n",
        "    if \"walk_distance2\" in df.columns:\n",
        "        score -= 0.10 * np.log1p(df[\"walk_distance2\"] + 1)\n",
        "\n",
        "    return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BBiTI04yq6SV"
      },
      "outputs": [],
      "source": [
        "# 擬似 price スコア\n",
        "df_train[\"pseudo_price\"] = make_pseudo_price(df_train)\n",
        "df_test[\"pseudo_price\"] = make_pseudo_price(df_test)\n",
        "\n",
        "# train の分位点で price_band を作る（重要）\n",
        "bins = pd.qcut(df_train[\"pseudo_price\"], q=5, retbins=True)[1]\n",
        "\n",
        "df_train[\"pseudo_price_band\"] = pd.cut(\n",
        "    df_train[\"pseudo_price\"], bins=bins, labels=False, include_lowest=True\n",
        ")\n",
        "\n",
        "df_test[\"pseudo_price_band\"] = pd.cut(\n",
        "    df_test[\"pseudo_price\"], bins=bins, labels=False, include_lowest=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia19Vz0mE02y"
      },
      "source": [
        "## addr1+2に対し、ターゲットエンコーディングを実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AYuXWy7YE25r"
      },
      "outputs": [],
      "source": [
        "# def target_encode_oof(\n",
        "#     df, col, target, n_splits=5, smoothing=10, random_state=42\n",
        "# ):\n",
        "#     df = df.copy()\n",
        "#     global_mean = df[target].mean()\n",
        "#     kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "\n",
        "#     te_col = np.zeros(len(df))\n",
        "\n",
        "#     for train_idx, valid_idx in kf.split(df):\n",
        "#         train_fold = df.iloc[train_idx]\n",
        "#         valid_fold = df.iloc[valid_idx]\n",
        "\n",
        "#         stats = train_fold.groupby(col)[target].agg([\"mean\", \"count\"])\n",
        "#         stats[\"smooth\"] = (\n",
        "#             stats[\"count\"] * stats[\"mean\"]\n",
        "#             + smoothing * global_mean\n",
        "#         ) / (stats[\"count\"] + smoothing)\n",
        "\n",
        "#         te_col[valid_idx] = valid_fold[col].map(stats[\"smooth\"])\n",
        "\n",
        "#     df[f\"{col}_TE\"] = te_col\n",
        "#     df[f\"{col}_TE\"].fillna(global_mean, inplace=True)\n",
        "\n",
        "#     return df, global_mean\n",
        "\n",
        "# from sklearn.model_selection import KFold\n",
        "# import numpy as np\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "def target_encode_oof(\n",
        "    df,\n",
        "    col,\n",
        "    target,\n",
        "    n_splits=5,\n",
        "    smoothing=10,\n",
        "    random_state=42\n",
        "):\n",
        "    df = df.copy()\n",
        "\n",
        "    # ★ここが最重要\n",
        "    df[target] = df[target].astype(float)\n",
        "    global_mean = df[target].mean()\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    te_col = np.zeros(len(df))\n",
        "\n",
        "    for train_idx, valid_idx in kf.split(df):\n",
        "        train_fold = df.iloc[train_idx]\n",
        "        valid_fold = df.iloc[valid_idx]\n",
        "\n",
        "        stats = (\n",
        "            train_fold\n",
        "            .groupby(col, observed=True)[target]\n",
        "            .agg([\"mean\", \"count\"])\n",
        "        )\n",
        "\n",
        "        stats[\"smooth\"] = (\n",
        "            stats[\"count\"] * stats[\"mean\"]\n",
        "            + smoothing * global_mean\n",
        "        ) / (stats[\"count\"] + smoothing)\n",
        "\n",
        "        te_col[valid_idx] = (\n",
        "            valid_fold[col]\n",
        "            .map(stats[\"smooth\"])\n",
        "            .astype(float)          # ← ★ここでカテゴリ解除\n",
        "            .fillna(global_mean)\n",
        "            .values\n",
        "          )\n",
        "\n",
        "\n",
        "    df[f\"{col}_TE\"] = te_col\n",
        "    return df, global_mean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F4UJgps5E4qu"
      },
      "outputs": [],
      "source": [
        "df_train, addr1_2_global_mean = target_encode_oof(\n",
        "    df_train,\n",
        "    col=\"addr1+2\",\n",
        "    target=\"money_room\",\n",
        "    n_splits=5,\n",
        "    smoothing=20\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_C1xkWTE7H9",
        "outputId": "585e5dbc-8d1b-4f26-fb73-e2e25f73bdaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-431935882.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  stats_full = df_train.groupby(\"addr1+2\")[\"money_room\"].agg([\"mean\", \"count\"])\n"
          ]
        }
      ],
      "source": [
        "stats_full = df_train.groupby(\"addr1+2\")[\"money_room\"].agg([\"mean\", \"count\"])\n",
        "\n",
        "stats_full[\"smooth\"] = (\n",
        "    stats_full[\"count\"] * stats_full[\"mean\"]\n",
        "    + 20 * addr1_2_global_mean\n",
        ") / (stats_full[\"count\"] + 20)\n",
        "\n",
        "# df_test[\"addr1+2_TE\"] = df_test[\"addr1+2\"].map(stats_full[\"smooth\"])\n",
        "# df_test[\"addr1+2_TE\"].fillna(addr1_2_global_mean, inplace=True)\n",
        "df_test[\"addr1+2_TE\"] = (\n",
        "    df_test[\"addr1+2\"]\n",
        "    .map(stats_full[\"smooth\"])\n",
        "    .astype(float)                 # ← ★最重要\n",
        "    .fillna(addr1_2_global_mean)   # ← inplace を使わない\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PDmg9UL3rAwe"
      },
      "outputs": [],
      "source": [
        "feature_list = feature_list + [\"pseudo_price_band\"]\n",
        "feature_list = feature_list + [\"addr1+2_TE\"]\n",
        "categorical_cols = categorical_cols + [\"pseudo_price_band\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Hs6paLwwBVsh"
      },
      "outputs": [],
      "source": [
        "# def make_sample_weight(pseudo_price_band):\n",
        "#     weight_map = {\n",
        "#         0: 8.4,   # 最低価格帯\n",
        "#         1: 1.0,\n",
        "#         2: 0.9,   # 標準\n",
        "#         3: 0.9,\n",
        "#         4: 1.2    # 高価格帯\n",
        "#     }\n",
        "#     return pseudo_price_band.map(weight_map).astype(float)\n",
        "\n",
        "# df_train[\"sample_weight\"] = make_sample_weight(\n",
        "#   df_train[\"pseudo_price_band\"]\n",
        "# )\n",
        "\n",
        "# 実際の売買価格で定義し直す\n",
        "def make_price_band_by_quantile(price, n_bins=5):\n",
        "    return pd.qcut(price, q=n_bins, labels=False)\n",
        "\n",
        "df_train[\"price_band\"] = make_price_band_by_quantile(\n",
        "    df_train[\"money_room\"],\n",
        "    n_bins=5\n",
        ")\n",
        "\n",
        "def make_sample_weight(price_band):\n",
        "    weight_map = {\n",
        "        0: 3.4,   # 最低価格帯\n",
        "        1: 1.0,\n",
        "        2: 0.9,\n",
        "        3: 0.9,\n",
        "        4: 1.2    # 高価格帯\n",
        "    }\n",
        "    return price_band.map(weight_map).astype(float)\n",
        "df_train[\"sample_weight\"] = make_sample_weight(\n",
        "    df_train[\"price_band\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNkth9VW2H_n"
      },
      "source": [
        "## 05.train/valid 分割　＆　target加工"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjO88LX_7nVW"
      },
      "source": [
        "＿★サンプルウェイトを価格帯別に調整可能とする"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDYt7poo2H_n"
      },
      "outputs": [],
      "source": [
        "# X_all, X_train, X_validの再構築\n",
        "X_all = df_train[feature_list]\n",
        "y_all = df_train[\"money_room\"]\n",
        "\n",
        "# log変換前の元価格を保存（後段の重み付け用）\n",
        "y_price_raw = df_train.loc[X_all.index, \"money_room\"]\n",
        "\n",
        "# unit_areaも対数変換\n",
        "X_all[\"unit_area\"] = np.log1p(X_all[\"unit_area\"])\n",
        "\n",
        "\n",
        "# unit_areaを２分割\n",
        "X_all_low = X_all[X_all[\"unit_area\"] < 4.5]\n",
        "X_all_high = X_all[X_all[\"unit_area\"] >= 4.5]\n",
        "df_train[\"unit_area\"] = np.log1p(df_train[\"unit_area\"])\n",
        "df_train_low = df_train[df_train[\"unit_area\"] < 4.5]\n",
        "df_train_high = df_train[df_train[\"unit_area\"] >= 4.5]\n",
        "\n",
        "\n",
        "\n",
        "# 目的変数が右に裾野が広いので対数変換\n",
        "y_all = np.log1p(y_all)\n",
        "y_all_low = y_all[X_all_low.index]\n",
        "y_all_high = y_all[X_all_high.index]\n",
        "\n",
        "X_train_low, X_valid_low, y_train_low, y_valid_low, w_train_low, w_valid_low = train_test_split(X_all_low, y_all_low, df_train_low[\"sample_weight\"], test_size=0.2, random_state=42)\n",
        "X_train_high, X_valid_high, y_train_high, y_valid_high, w_train_high, w_valid_high = train_test_split(X_all_high, y_all_high, df_train_high[\"sample_weight\"], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5lPFsjA2H_o"
      },
      "source": [
        "## 06.sample_weight適用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orbb7AzF2H_o"
      },
      "source": [
        "## 07.モデル学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UzfPSES7je"
      },
      "outputs": [],
      "source": [
        "# カスタム評価関数（eval_metric形式）\n",
        "def mape_eval(preds, train_data):\n",
        "    y_true = np.expm1(train_data.get_label())\n",
        "    y_pred = np.expm1(preds)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-7))) * 100\n",
        "    return 'mape', mape, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UthK6de1T6Ty"
      },
      "outputs": [],
      "source": [
        "# LightGBM のパラメータ設定\n",
        "params = config['MODEL_PARAMS']\n",
        "\n",
        "# LightGBM のデータセットを作成\n",
        "lgb_train_low = lgb.Dataset(\n",
        "    X_train_low,\n",
        "    y_train_low,\n",
        "    # weight=sample_weight.loc[X_train.index],\n",
        "    weight=w_train_low,\n",
        "    categorical_feature=categorical_cols\n",
        ")\n",
        "lgb_train_high = lgb.Dataset(\n",
        "    X_train_high,\n",
        "    y_train_high,\n",
        "    # weight=sample_weight.loc[X_train.index],\n",
        "    weight=w_train_high,\n",
        "    categorical_feature=categorical_cols\n",
        ")\n",
        "\n",
        "lgb_test_low = lgb.Dataset(\n",
        "    X_valid_low,\n",
        "    y_valid_low,\n",
        "    reference=lgb_train_low,\n",
        "    categorical_feature=categorical_cols\n",
        ")\n",
        "lgb_test_high = lgb.Dataset(\n",
        "    X_valid_high,\n",
        "    y_valid_high,\n",
        "    reference=lgb_train_high,\n",
        "    categorical_feature=categorical_cols\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4dTzGRyA2XQ"
      },
      "outputs": [],
      "source": [
        "# モデルの学習\n",
        "model_low = lgb.train(params,\n",
        "                  lgb_train_low,\n",
        "                  valid_sets=[lgb_train_low, lgb_test_low],\n",
        "                  feval=mape_eval,  # ← カスタム評価関数を指定\n",
        "                  callbacks=[lgb.early_stopping(stopping_rounds=1000, verbose=False)\n",
        "                  ]) #early_stoppingあり\n",
        "model_high = lgb.train(params,\n",
        "                  lgb_train_high,\n",
        "                  valid_sets=[lgb_train_high, lgb_test_high],\n",
        "                  feval=mape_eval,  # ← カスタム評価関数を指定\n",
        "                  callbacks=[lgb.early_stopping(stopping_rounds=1000, verbose=False)\n",
        "                  ]) #early_stoppingあり\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ_AzEqc2H_p"
      },
      "source": [
        "## 08.評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ8MsZmS2H_p"
      },
      "outputs": [],
      "source": [
        "# テストデータで予測\n",
        "y_pred_low = model_low.predict(X_valid_low , num_iteration=model_low.best_iteration)\n",
        "y_pred_high = model_high.predict(X_valid_high , num_iteration=model_high.best_iteration)\n",
        "\n",
        "# 対数変換を戻す\n",
        "y_pred_low = np.expm1(y_pred_low)\n",
        "y_valid_low = np.expm1(y_valid_low)\n",
        "y_pred_high = np.expm1(y_pred_high)\n",
        "y_valid_high = np.expm1(y_valid_high)\n",
        "\n",
        "# yp = pd.DataFrame(y_pred,columns=[\"%\"]) # ???????????????????????????//\n",
        "\n",
        "\n",
        "# 特徴量の重要度\n",
        "print(\"特徴量の重要度\")\n",
        "lgb.plot_importance(model_low, figsize=(8,4))\n",
        "lgb.plot_importance(model_high, figsize=(8,4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR1rdmTd2H_p"
      },
      "source": [
        "## 09.可視化（importance SHAP）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5HVNvdJA7rn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import lightgbm as lgb\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # パラメータの探索範囲を指定\n",
        "# param_grid = {\n",
        "#     'num_leaves': [20, 30, 40],\n",
        "#     'learning_rate': [0.01, 0.1, 0.5],\n",
        "#     'max_depth': [5, 10]\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # グリッドサーチCV\n",
        "# gsearch = GridSearchCV(gbm, param_grid, cv=5) #cvは交差検証の回数\n",
        "\n",
        "# # データを学習\n",
        "# gsearch.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "\n",
        "\n",
        "\n",
        "# # 最適なパラメータとスコアを表示\n",
        "# print('Best parameters found by grid search are:', gsearch.best_params_)\n",
        "# print('Best score:', gsearch.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqnAa0aHUkSF"
      },
      "outputs": [],
      "source": [
        "# # パラメータの辞書を結合\n",
        "# best_params = {**params, **gsearch.best_params_}\n",
        "\n",
        "# # 最適パラメータでモデルを再学習\n",
        "# model = lgb.LGBMClassifier(**best_params)\n",
        "# model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtosh8APQ1hX"
      },
      "outputs": [],
      "source": [
        "#SHAP値の取得\n",
        "explainer_low = shap.TreeExplainer(model=model_low)#SHAP値を取得するためのモデル作成\n",
        "shap_values_low = explainer_low.shap_values(X=X_valid_low)#説明変数それぞれの値のSHAP値を取得する\n",
        "explainer_high = shap.TreeExplainer(model=model_high)#SHAP値を取得するためのモデル作成\n",
        "shap_values_high = explainer_high.shap_values(X=X_valid_high)#説明変数それぞれの値のSHAP値を取得する\n",
        "\n",
        "# print(shap_values)\n",
        "# print(shap_values.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHCG_cntQ60c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 変数別の影響度の可視化\n",
        "shap.initjs()\n",
        "# shap.summary_plot(shap_values_low, X_valid_low, plot_type=\"bar\", show=False)\n",
        "shap.summary_plot(shap_values_low, X_valid_low, show=False)\n",
        "shap.summary_plot(shap_values_high, X_valid_high, show=False)\n",
        "\n",
        "# 0番目のデータポイントを再選択\n",
        "i = 0\n",
        "single_observation_low = X_valid_low.iloc[i:i+1,:]\n",
        "single_observation_high = X_valid_high.iloc[i:i+1,:]\n",
        "\n",
        "#print(single_observation)\n",
        "\n",
        "\n",
        "# Explainerを使って説明を再計算\n",
        "single_shap_values_low = explainer_low(single_observation_low)\n",
        "single_shap_values_high = explainer_high(single_observation_high)\n",
        "\n",
        "# waterfallプロットの生成\n",
        "shap.waterfall_plot(single_shap_values_low[0])\n",
        "shap.waterfall_plot(single_shap_values_high[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ==============================\n",
        "# 定数\n",
        "# ==============================\n",
        "UNIT_AREA_TH = 4.5\n",
        "MAX_LOG = 15\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ==============================\n",
        "# 安全関数\n",
        "# ==============================\n",
        "def safe_expm1(x):\n",
        "    x = np.asarray(x, dtype=\"float64\")\n",
        "    x = np.clip(x, -10, MAX_LOG)\n",
        "    return np.expm1(x)\n",
        "\n",
        "def safe_mape(y_true, y_pred):\n",
        "    y_true = np.asarray(y_true, dtype=\"float64\")\n",
        "    y_pred = np.asarray(y_pred, dtype=\"float64\")\n",
        "    mask = (\n",
        "        (y_true > 0) &\n",
        "        np.isfinite(y_true) &\n",
        "        np.isfinite(y_pred)\n",
        "    )\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
        "\n",
        "# ==============================\n",
        "# ① データ読み込み\n",
        "# ==============================\n",
        "#df_train = pd.read_csv(\"train.csv\")\n",
        "#df_test  = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# ==============================\n",
        "# ② 特徴量・目的変数\n",
        "# ==============================\n",
        "X_all = df_train.loc[:, feature_list].copy()\n",
        "y_all = df_train[\"money_room\"].copy()\n",
        "\n",
        "# unit_area log変換（view回避）\n",
        "X_all.loc[:, \"unit_area\"] = np.log1p(X_all[\"unit_area\"])\n",
        "\n",
        "# 目的変数 log変換\n",
        "y_all = np.log1p(y_all)\n",
        "\n",
        "# ==============================\n",
        "# ③ low / high 分割（index維持）\n",
        "# ==============================\n",
        "mask_low  = X_all[\"unit_area\"] < UNIT_AREA_TH\n",
        "mask_high = X_all[\"unit_area\"] >= UNIT_AREA_TH\n",
        "\n",
        "X_all_low  = X_all.loc[mask_low]\n",
        "X_all_high = X_all.loc[mask_high]\n",
        "\n",
        "y_all_low  = y_all.loc[X_all_low.index]\n",
        "y_all_high = y_all.loc[X_all_high.index]\n",
        "\n",
        "w_all_low  = df_train.loc[X_all_low.index,  \"sample_weight\"]\n",
        "w_all_high = df_train.loc[X_all_high.index, \"sample_weight\"]\n",
        "\n",
        "# ==============================\n",
        "# ④ train / valid 分割\n",
        "# ==============================\n",
        "X_train_low, X_valid_low, y_train_low, y_valid_low, w_train_low, w_valid_low = train_test_split(\n",
        "    X_all_low, y_all_low, w_all_low,\n",
        "    test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "X_train_high, X_valid_high, y_train_high, y_valid_high, w_train_high, w_valid_high = train_test_split(\n",
        "    X_all_high, y_all_high, w_all_high,\n",
        "    test_size=0.2, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# ⑤ モデル学習\n",
        "# ==============================\n",
        "model_low.fit(\n",
        "    X_train_low, y_train_low,\n",
        "    sample_weight=w_train_low\n",
        ")\n",
        "\n",
        "model_high.fit(\n",
        "    X_train_high, y_train_high,\n",
        "    sample_weight=w_train_high\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# ⑥ valid MAPE 評価\n",
        "# ==============================\n",
        "y_pred_valid_low  = safe_expm1(model_low.predict(X_valid_low))\n",
        "y_true_valid_low  = safe_expm1(y_valid_low)\n",
        "\n",
        "y_pred_valid_high = safe_expm1(model_high.predict(X_valid_high))\n",
        "y_true_valid_high = safe_expm1(y_valid_high)\n",
        "\n",
        "pred_low  = pd.Series(y_pred_valid_low,  index=X_valid_low.index)\n",
        "pred_high = pd.Series(y_pred_valid_high, index=X_valid_high.index)\n",
        "\n",
        "true_low  = pd.Series(y_true_valid_low,  index=X_valid_low.index)\n",
        "true_high = pd.Series(y_true_valid_high, index=X_valid_high.index)\n",
        "\n",
        "y_pred_valid_all = pd.concat([pred_low, pred_high]).sort_index().values\n",
        "y_true_valid_all = pd.concat([true_low, true_high]).sort_index().values\n",
        "\n",
        "mape_valid = safe_mape(y_true_valid_all, y_pred_valid_all)\n",
        "print(f\"valid MAPE: {mape_valid * 100:.2f}%\")\n",
        "\n",
        "# ==============================\n",
        "# ⑦ テストデータ前処理\n",
        "# ==============================\n",
        "X_test = df_test.loc[:, feature_list].copy()\n",
        "X_test.loc[:, \"unit_area\"] = np.log1p(X_test[\"unit_area\"])\n",
        "\n",
        "mask_test_low  = X_test[\"unit_area\"] < UNIT_AREA_TH\n",
        "mask_test_high = X_test[\"unit_area\"] >= UNIT_AREA_TH\n",
        "\n",
        "X_test_low  = X_test.loc[mask_test_low]\n",
        "X_test_high = X_test.loc[mask_test_high]\n",
        "\n",
        "# ==============================\n",
        "# ⑧ テスト予測\n",
        "# ==============================\n",
        "y_test_pred_low  = safe_expm1(model_low.predict(X_test_low))\n",
        "y_test_pred_high = safe_expm1(model_high.predict(X_test_high))\n",
        "\n",
        "pred_test_low  = pd.Series(y_test_pred_low,  index=X_test_low.index)\n",
        "pred_test_high = pd.Series(y_test_pred_high, index=X_test_high.index)\n",
        "\n",
        "# ==============================\n",
        "# ⑨ 提出用 y_submit（順序完全維持）\n",
        "# ==============================\n",
        "y_submit = pd.concat([pred_test_low, pred_test_high]).sort_index()\n",
        "\n",
        "# 数値のみ（Series）\n",
        "y_submit = y_submit.astype(\"float64\")\n",
        "\n",
        "print(\"submit shape:\", y_submit.shape)\n",
        "print(y_submit.head())"
      ],
      "metadata": {
        "id": "ZkwIoRihIL9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQfAs_kZ2H_v"
      },
      "source": [
        "## 10.推論、提出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxCenyIHz0GT"
      },
      "source": [
        "## 提出用データの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G20PdnIGVCgo"
      },
      "outputs": [],
      "source": [
        "# 特徴量の選定\n",
        "# df_test_p = df_test[feature_list].copy()\n",
        "# df_test_p[\"unit_area\"] = np.log1p(df_test_p[\"unit_area\"])\n",
        "\n",
        "# df_test_p_low = df_test_p[df_test_p[\"unit_area\"] < 4.5]\n",
        "# df_test_p_high = df_test_p[df_test_p[\"unit_area\"] >= 4.5]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # 提出データに対する予測（確率値）\n",
        "# y_scores_submit_low = model_low.predict(df_test_p_low)\n",
        "# y_scores_submit_low= np.expm1(y_scores_submit_low)\n",
        "# y_scores_submit_high = model_high.predict(df_test_p_high)\n",
        "# y_scores_submit_high= np.expm1(y_scores_submit_high)\n",
        "\n",
        "# 元の index を保持\n",
        "df_test_p = df_test[feature_list].copy()\n",
        "df_test_p[\"unit_area\"] = np.log1p(df_test_p[\"unit_area\"])\n",
        "\n",
        "# 分割（indexは保持される）\n",
        "df_test_p_low  = df_test_p[df_test_p[\"unit_area\"] < 4.5]\n",
        "df_test_p_high = df_test_p[df_test_p[\"unit_area\"] >= 4.5]\n",
        "\n",
        "# 予測\n",
        "y_pred_low  = np.expm1(model_low.predict(df_test_p_low))\n",
        "y_pred_high = np.expm1(model_high.predict(df_test_p_high))\n",
        "\n",
        "# ★ index 付き Series にする\n",
        "pred_low  = pd.Series(y_pred_low,  index=df_test_p_low.index)\n",
        "pred_high = pd.Series(y_pred_high, index=df_test_p_high.index)\n",
        "\n",
        "# ★ 空の Series を作って元の index で埋める\n",
        "y_submit = pd.Series(index=df_test_p.index, dtype=float)\n",
        "y_submit.loc[pred_low.index]  = pred_low\n",
        "y_submit.loc[pred_high.index] = pred_high\n",
        "\n",
        "# idxを除いた数値を抽出\n",
        "y_scores_submit = y_submit.values\n",
        "\n",
        "\n",
        "print(y_scores_submit)\n",
        "# print(y_scores_submit_low)\n",
        "# print(y_scores_submit_high)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル別のMAPEを確認\n",
        "\n",
        "\n",
        "y_pred_valid_low  = safe_expm1(model_low.predict(X_valid_low))\n",
        "y_pred_valid_high = safe_expm1(model_high.predict(X_valid_high))\n",
        "\n",
        "y_true_valid_low  = np.expm1(y_valid_low)\n",
        "y_true_valid_high = np.expm1(y_valid_high)\n",
        "def safe_mape(y_true, y_pred):\n",
        "    mask = (y_true > 0) & np.isfinite(y_pred)\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
        "\n",
        "pred_low  = pd.Series(y_pred_valid_low,  index=X_valid_low.index)\n",
        "pred_high = pd.Series(y_pred_valid_high, index=X_valid_high.index)\n",
        "\n",
        "true_low  = pd.Series(y_true_valid_low,  index=X_valid_low.index)\n",
        "true_high = pd.Series(y_true_valid_high, index=X_valid_high.index)\n",
        "\n",
        "y_pred_valid_all = pd.concat([pred_low, pred_high]).sort_index()\n",
        "y_true_valid_all = pd.concat([true_low, true_high]).sort_index()\n",
        "\n",
        "mape_valid = safe_mape(y_true_valid_all, y_pred_valid_all)\n",
        "print(f\"valid MAPE: {mape_valid * 100:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# pred_low  = pd.Series(y_pred_valid_low,  index=X_valid_low.index)\n",
        "# pred_high = pd.Series(y_pred_valid_high, index=X_valid_high.index)\n",
        "\n",
        "# true_low  = pd.Series(y_true_valid_low,  index=X_valid_low.index)\n",
        "# true_high = pd.Series(y_true_valid_high, index=X_valid_high.index)\n",
        "\n",
        "# y_pred_valid_all = pd.concat([pred_low, pred_high]).sort_index()\n",
        "# y_true_valid_all = pd.concat([true_low, true_high]).sort_index()\n",
        "# assert y_pred_valid_all.index.equals(y_true_valid_all.index)\n",
        "\n",
        "# mape_valid = np.mean(\n",
        "#     np.abs((y_true_valid_all - y_pred_valid_all) / y_true_valid_all)\n",
        "# )\n",
        "\n",
        "# print(f\"valid MAPE: {mape_valid * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# mask = y_true_valid_all > 0\n",
        "\n",
        "# mape_valid = np.mean(\n",
        "#     np.abs(\n",
        "#         (y_true_valid_all[mask] - y_pred_valid_all[mask])\n",
        "#         / y_true_valid_all[mask]\n",
        "#     )\n",
        "# )\n",
        "\n",
        "# print(f\"valid MAPE: {mape_valid * 100:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3fpY65p5nFCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lWNqV26CjYJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#提出用csvの作成\n",
        "df_scores_submit = pd.DataFrame(y_scores_submit)\n",
        "\n",
        "# df_submit = pd.concat([df_test[\"id\"], df_scores_submit], axis=1)\n",
        "# df_scores_submit.index = df_scores_submit.index + 1\n",
        "df_scores_submit.to_csv(\"submit.csv\", index=True, header=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZUHToQRPbBJ"
      },
      "source": [
        "## 誤差要因分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE3uPKSQPdVh"
      },
      "outputs": [],
      "source": [
        "# valid予測\n",
        "y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "# 対数変換を戻す\n",
        "y_pred_valid = np.expm1(y_pred_valid)\n",
        "\n",
        "df_eval = X_valid.copy()\n",
        "df_eval[\"y_true\"] = y_valid\n",
        "df_eval[\"y_pred\"] = y_pred_valid\n",
        "\n",
        "# APE計算（0割防止）\n",
        "df_eval[\"ape\"] = np.abs(df_eval[\"y_true\"] - df_eval[\"y_pred\"]) / np.maximum(df_eval[\"y_true\"], 1e-7)\n",
        "\n",
        "# 上位ワースト確認\n",
        "# df_eval.sort_values(\"ape\", ascending=False).head(20)\n",
        "df_eval[\"スコア差分\"] = df_eval[\"y_true\"] - df_eval[\"y_pred\"]\n",
        "df_eval[\"スコア差分\"].plot.hist(bins=50, figsize=(10,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVqE_gmXPgoo"
      },
      "outputs": [],
      "source": [
        "# 価格帯ビン作成\n",
        "df_eval[\"price_bin\"] = pd.qcut(df_eval[\"y_true\"], q=5)\n",
        "\n",
        "# 価格帯別MAPE\n",
        "mape_by_bin = df_eval.groupby(\"price_bin\")[\"ape\"].mean() * 100\n",
        "print(\"価格帯別のMAPE\\n\", mape_by_bin)\n",
        "print()\n",
        "print(\"全データのMAPE\", df_eval[\"ape\"].mean())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPfb_aJJPmdK"
      },
      "outputs": [],
      "source": [
        "def compare_distribution(col):\n",
        "    return pd.DataFrame({\n",
        "        \"train\": df_train[col].describe(),\n",
        "        \"valid\": X_valid[col].describe()\n",
        "    })\n",
        "\n",
        "# compare_distribution(\"money_rimawari_now\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65k3FdkrVQVN"
      },
      "outputs": [],
      "source": [
        "# APEが大きい上位10%\n",
        "threshold = df_eval[\"ape\"].quantile(0.9)\n",
        "bad_samples = df_eval[df_eval[\"ape\"] >= threshold]\n",
        "\n",
        "# SHAP値抽出\n",
        "shap_values_valid = explainer.shap_values(X_valid)\n",
        "shap_df = pd.DataFrame(\n",
        "    shap_values_valid,\n",
        "    columns=X_valid.columns,\n",
        "    index=X_valid.index\n",
        ")\n",
        "\n",
        "\n",
        "# 悪いサンプルのSHAP平均\n",
        "shap_df.loc[bad_samples.index].abs().mean().sort_values(ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GcuSM9FVYZK"
      },
      "outputs": [],
      "source": [
        "# shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values, features=X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUrvXmwxm9Qw"
      },
      "outputs": [],
      "source": [
        "def calc_mape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-7))) * 100\n",
        "\n",
        "\n",
        "# valid 予測\n",
        "y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "# 対数変換を戻す\n",
        "y_pred_valid = np.expm1(y_pred_valid)\n",
        "\n",
        "# MAPE 出力\n",
        "valid_mape = calc_mape(y_valid, y_pred_valid)\n",
        "print(f\"VALID MAPE: {valid_mape:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eicRmSRU2H_x"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df_eval = X_valid.copy()\n",
        "df_eval[\"unit_area\"] = np.expm1(df_eval[\"unit_area\"])\n",
        "df_eval[\"y_true\"] = y_valid\n",
        "df_eval[\"y_pred\"] = y_pred_valid\n",
        "df_eval[\"ape\"] = np.abs(df_eval[\"y_true\"] - df_eval[\"y_pred\"]) / np.maximum(df_eval[\"y_true\"], 1e-7)\n",
        "\n",
        "# 上位10%の誤差サンプル抽出\n",
        "threshold = df_eval[\"ape\"].quantile(0.9)\n",
        "bad_samples = df_eval[df_eval[\"ape\"] >= threshold]\n",
        "\n",
        "# 調査対象の特徴量リスト\n",
        "check_features = [\n",
        "    \"unit_area\",\n",
        "    \"post1\",\n",
        "    \"floor_plan_code\",\n",
        "    \"walk_distance1\",\n",
        "    \"walk_distance2\",\n",
        "]\n",
        "\n",
        "# タグ系カラムを抽出（feature_list に基づく）\n",
        "tag_features = [col for col in feature_list if col.startswith(\"tag_\")]\n",
        "check_features.extend(tag_features)\n",
        "\n",
        "# 1. 数値特徴量の誤差 vs 値域の関係\n",
        "num_features = [\"unit_area\", \"walk_distance1\", \"walk_distance2\"]\n",
        "for col in num_features:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.scatterplot(data=df_eval, x=col, y=\"ape\", alpha=0.3)\n",
        "    sns.scatterplot(data=bad_samples, x=col, y=\"ape\", color=\"red\", alpha=0.5)\n",
        "    plt.title(f\"{col} と予測誤差(APE)の関係\")\n",
        "    plt.ylabel(\"APE\")\n",
        "    plt.xlabel(col)\n",
        "    plt.show()\n",
        "\n",
        "# 2. カテゴリ特徴量の誤差分布（箱ひげ図）\n",
        "cat_features = [\"post1\", \"floor_plan_code\"]\n",
        "for col in cat_features:\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.boxplot(x=col, y=\"ape\", data=df_eval)\n",
        "    plt.title(f\"{col} ごとの予測誤差(APE)分布\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# 3. タグ系特徴量の影響（悪いサンプルと全体比較）\n",
        "for col in tag_features:\n",
        "    if col not in df_eval.columns:\n",
        "        continue\n",
        "    mean_all = df_eval[col].mean()\n",
        "    mean_bad = bad_samples[col].mean()\n",
        "    print(f\"{col}: 全体平均={mean_all:.3f}, 誤差上位10%平均={mean_bad:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}