{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A8CzKrBbb7O",
        "outputId": "180bc5df-4977-496f-ee20-9e1c9924fe28"
      },
      "outputs": [],
      "source": [
        "# !pip install japanize-matplotlib\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification                         \n",
        "import japanize_matplotlib #日本語表示対応\n",
        "\n",
        "\n",
        "# Notebook から src ディレクトリを追加\n",
        "# sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
        "sys.path.append(\"/home/keiseki/My_Python_project/src\")\n",
        "\n",
        "# これで src/preprocess/make_tag_features.py が import 可能\n",
        "# from preprocess.make_tag_features import create_tag_features, extract_unique_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "csR9wtPUwBcd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=pd.errors.PerformanceWarning\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 01.config読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "conf_path = os.path.join( '../config/config.yaml')\n",
        "with open(conf_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定義した特徴量リストを読み込み\n",
        "feature_list = config['FEATURE']['FEATURE_LIST']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# カテゴリカルカラムのリストを定義\n",
        "categorical_cols = config['FEATURE']['CATEGORICAL_COLS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 02.データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLIBSLbkZpre",
        "outputId": "c396ab8d-e46f-4d3e-ef87-c4a3441a5e39"
      },
      "outputs": [
        {
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ドライブ内のファイルパスを指定\u001b[39;00m\n\u001b[32m      2\u001b[39m train_path = \u001b[33m'\u001b[39m\u001b[33m../20.Data/processed_train.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_train = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
          ]
        }
      ],
      "source": [
        "# ドライブ内のファイルパスを指定\n",
        "train_path = '../20.Data/processed_train.pkl'\n",
        "df_train = pd.read_pickle(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4188/2594490984.py:3: DtypeWarning: Columns (46,55,56,63,146) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_test = pd.read_csv(test_path)\n"
          ]
        }
      ],
      "source": [
        "# testデータの読み込み\n",
        "test_path = '../20.Data/processed_test.pkl'\n",
        "df_test = pd.read_pickle(test_path)\n",
        "\n",
        "# df_test[\"renovation_date\"] = pd.to_datetime(df_test[\"renovation_date\"])\n",
        "# df_test[\"renovation_date\"] = df_test[\"renovation_date\"].astype(\"int64\") // 10**9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03.前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# # df_trainのカテゴリカルカラムを'category'型に変換\n",
        "# for col in categorical_cols:\n",
        "#     if col in df_train.columns:\n",
        "#         df_train[col] = df_train[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # df_testのカテゴリカルカラムを'category'型に変換\n",
        "# for col in categorical_cols:\n",
        "#     if col in df_test.columns:\n",
        "#         df_test[col] = df_test[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 極端に高い売買価格はクリッピング\n",
        "# lower = df_train[\"money_room\"].quantile(0.01)\n",
        "# upper = df_train[\"money_room\"].quantile(0.99)\n",
        "\n",
        "# df_train[\"money_room\"] = df_train[\"money_room\"].clip(lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 極端な値はクリッピング\n",
        "# for col in feature_list:\n",
        "#     # 数値型でなければスキップ\n",
        "#     if not pd.api.types.is_numeric_dtype(df_train[col]):\n",
        "#         continue\n",
        "\n",
        "#     # 全部NaNの列はスキップ\n",
        "#     if df_train[col].notna().sum() == 0:\n",
        "#         continue\n",
        "\n",
        "#     # 1% / 99% パーセンタイル\n",
        "#     lower = df_train[col].quantile(0.01)\n",
        "#     upper = df_train[col].quantile(0.99)\n",
        "\n",
        "#     # パーセンタイルが計算できない場合はスキップ\n",
        "#     if pd.isna(lower) or pd.isna(upper):\n",
        "#         continue\n",
        "\n",
        "#     # クリッピング\n",
        "#     df_train[col] = df_train[col].clip(lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_train[\"land_youto\"] = df_train[\"land_youto\"].where(\n",
        "#     df_train[\"land_youto\"].isin([8, 14]),\n",
        "#     np.nan\n",
        "# ).astype(\"category\")\n",
        "\n",
        "# df_test[\"land_youto\"] = df_test[\"land_youto\"].where(\n",
        "#     df_test[\"land_youto\"].isin([8, 14]),\n",
        "#     np.nan\n",
        "# ).astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_train[\"building_type\"] = (\n",
        "#     df_train[\"building_type\"]\n",
        "#     .where(df_train[\"building_type\"].isin([1, 3]), np.nan)\n",
        "# )\n",
        "\n",
        "# df_test[\"building_type\"] = (\n",
        "#     df_test[\"building_type\"]\n",
        "#     .where(df_test[\"building_type\"].isin([1, 3]), np.nan)\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 欠損値はカラムの型に応じて補完\n",
        "# for col in df_train.columns:\n",
        "#     if col == \"money_room\": # 目的変数はこの時点では補完しない\n",
        "#         continue\n",
        "\n",
        "#     # if col in categorical_cols: # 明示的に定義されたカテゴリカルカラム\n",
        "#     #     # LightGBMが0をカテゴリとして扱えるため、0で補完\n",
        "#     #     df_train[col] = df_train[col].fillna(0)\n",
        "#     elif df_train[col].dtype in ['int64', 'float64']: # 数値カラム\n",
        "#         # 数値カラムは訓練データの中央値で補完\n",
        "#         median_val = df_train[col].median()\n",
        "#         df_train[col] = df_train[col].fillna(median_val)\n",
        "#         df_test[col] = df_test[col].fillna(median_val)\n",
        "#     # else:\n",
        "#         # # その他の型（objectなど）は、とりあえず0で補完（必要に応じて調整）\n",
        "#         # df_train[col] = df_train[col].fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # # room_count\n",
        "# # # unit_area\n",
        "# df_train = df_train[(df_train[\"unit_area\"]>12.5) & (df_train[\"unit_area\"]<300)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ##### 日付関係のデータは経過日数を特徴量とする -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # 'target_ym'をdatetime型に変換 (月の最初の日と仮定)\n",
        "# df_train['target_date'] = pd.to_datetime(df_train['target_ym'].astype(str) + '01', format='%Y%m%d')\n",
        "# df_test['target_date'] = pd.to_datetime(df_test['target_ym'].astype(str) + '01', format='%Y%m%d')\n",
        "\n",
        "# # 'building_create_date'をdatetime型に変換\n",
        "# df_train['building_create_date'] = pd.to_datetime(df_train['building_create_date'])\n",
        "# df_test['building_create_date'] = pd.to_datetime(df_test['building_create_date'])\n",
        "# # 築年数を計算 (年単位)\n",
        "# df_train['building_age'] = (df_train['target_date'] - df_train['building_create_date']).dt.days // 365\n",
        "# df_test['building_age'] = (df_test['target_date'] - df_test['building_create_date']).dt.days // 365\n",
        "\n",
        "# # 'renovation_date'がUNIXタイムスタンプになっているため、再度datetimeに変換して経過年数を計算\n",
        "# # 既に'renovation_date'はUNIXタイムスタンプに変換済みのため、元の日付情報に戻すか、元のカラムを使用する\n",
        "# # ここでは、元のカラムが保持されていると仮定し、新しいカラムとして計算\n",
        "# # 注: もしdf_train['renovation_date']がUNIXタイムスタンプのみになっている場合、この処理は調整が必要です。\n",
        "# # 既存のrenovation_date（UNIXタイムスタンプ）を無視し、元の文字列カラムが存在しないため、新たに変換する\n",
        "# # もし元の文字列カラムが失われている場合は、UNIXタイムスタンプをdatetimeに変換する\n",
        "# # 現状のNotebookの処理から判断すると、renovation_dateはUNIXタイムスタンプなので、それを基に経過年数を計算します。\n",
        "\n",
        "# # UNIXタイムスタンプからdatetimeへの変換\n",
        "# df_train['renovation_datetime'] = pd.to_datetime(df_train['renovation_date'], errors='coerce')\n",
        "# df_test['renovation_datetime'] = pd.to_datetime(df_test['renovation_date'], errors='coerce')\n",
        "\n",
        "# # リノベーションからの経過年数を計算 (年単位)\n",
        "# # リノベーション日がない場合はNaNとなり、NaNとtarget_dateの差もNaNになるため、後でfillnaで処理\n",
        "# df_train['years_since_renovation'] = (df_train['target_date'] - df_train['renovation_datetime']).dt.days // 365\n",
        "# df_test['years_since_renovation'] = (df_test['target_date'] - df_test['renovation_datetime']).dt.days // 365\n",
        "# # 欠損値の補完\n",
        "# # building_ageとyears_since_renovationの負の値（未来の日付）やNaNを処理\n",
        "# # 例: 負の値やNaNを0で補完する\n",
        "# df_train['building_age'] = df_train['building_age'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "# df_test['building_age'] = df_test['building_age'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "\n",
        "# df_train['years_since_renovation'] = df_train['years_since_renovation'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "# df_test['years_since_renovation'] = df_test['years_since_renovation'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "# # 新しい特徴量をfeature_listに追加\n",
        "# new_features = ['building_age', 'years_since_renovation']\n",
        "# feature_list.extend(new_features)\n",
        "\n",
        "# # feature_listから'target_date', 'building_create_date', 'renovation_datetime'を削除 (直接特徴量として使わないため)\n",
        "# # (元のbuilding_create_dateとrenovation_dateはUNIX timestampとして残しておく)\n",
        "# feature_list = [f for f in feature_list if f not in ['target_date', 'building_create_date', 'renovation_date', 'renovation_datetime']]\n",
        "\n",
        "# # 確認\n",
        "# # print(df_train[feature_list].head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- ## 04.特徴量生成 -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # タグIDの分割により特徴量を追加\n",
        "\n",
        "# # all_unique_tags という空の set を初期化\n",
        "# all_unique_tags = set()\n",
        "# # df_train の 'statuses' カラムに対して extract_unique_tags を呼び出し、得られたユニークタグを追加\n",
        "# all_unique_tags.update(extract_unique_tags(df_train, 'statuses'))\n",
        "# # df_test の 'statuses' カラムに対して extract_unique_tags を呼び出し、得られたユニークタグを追加\n",
        "# all_unique_tags.update(extract_unique_tags(df_test, 'statuses'))\n",
        "# # all_unique_tags をソートしてリストに変換し、all_unique_tags_list に格納\n",
        "# all_unique_tags_list = sorted(list(all_unique_tags))\n",
        "\n",
        "# # create_tag_features 関数を df_train と df_test に適用\n",
        "# df_train, new_tag_features = create_tag_features(df_train.copy(), all_unique_tags_list)\n",
        "# df_test, _ = create_tag_features(df_test.copy(), all_unique_tags_list)\n",
        "# # 既存の feature_list に new_tag_features を追加\n",
        "# feature_list.extend(new_tag_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # LightGBM 用に使用するカテゴリ特徴量のリストを作成\n",
        "# categorical_features = [\n",
        "#     col for col in categorical_cols\n",
        "#     if col in feature_list\n",
        "# ]\n",
        "\n",
        "# feature_list = feature_list.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 05.train/valid 分割　＆　target加工"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_4188/4294696482.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X_all[\"unit_area\"] = np.log1p(X_all[\"unit_area\"])\n"
          ]
        }
      ],
      "source": [
        "# X_all, X_train, X_validの再構築\n",
        "X_all = df_train[feature_list]\n",
        "y_all = df_train[\"money_room\"]\n",
        "\n",
        "# log変換前の元価格を保存（後段の重み付け用）\n",
        "y_price_raw = df_train.loc[X_all.index, \"money_room\"]\n",
        "\n",
        "# unit_areaも対数変換\n",
        "X_all[\"unit_area\"] = np.log1p(X_all[\"unit_area\"])\n",
        "\n",
        "# 目的変数が右に裾野が広いので対数変換\n",
        "y_all = np.log1p(y_all)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 06.sample_weight適用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 元スケールの価格\n",
        "y_price = y_all\n",
        "\n",
        "\n",
        "# train / valid に合わせる\n",
        "y_price_raw_train = y_price_raw.loc[X_train.index]\n",
        "\n",
        "# 低価格ほど重く（価格の逆数）\n",
        "sample_weight = 1 / np.log1p(np.maximum(y_price_raw_train, 1_000_000)* np.where(y_price_raw_train < 13_000_000, 2.0, 1.0) )\n",
        "\n",
        "# 正規化\n",
        "sample_weight = sample_weight / sample_weight.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 07.モデル学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UzfPSES7je"
      },
      "outputs": [],
      "source": [
        "# カスタム評価関数（eval_metric形式）\n",
        "def mape_eval(preds, train_data):\n",
        "    y_true = np.expm1(train_data.get_label())\n",
        "    y_pred = np.expm1(preds)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-7))) * 100\n",
        "    return 'mape', mape, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UthK6de1T6Ty"
      },
      "outputs": [],
      "source": [
        "# LightGBM のパラメータ設定\n",
        "params = config['MODEL_PARAMS']\n",
        "\n",
        "# LightGBM のデータセットを作成\n",
        "lgb_train = lgb.Dataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    weight=sample_weight.loc[X_train.index],\n",
        "    categorical_feature=categorical_cols\n",
        ")\n",
        "\n",
        "lgb_test = lgb.Dataset(\n",
        "    X_valid,\n",
        "    y_valid,\n",
        "    reference=lgb_train,\n",
        "    categorical_feature=categorical_cols\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "collapsed": true,
        "id": "V4dTzGRyA2XQ",
        "outputId": "a0328312-2095-4eb2-99d7-976e73a1d28d"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: addr1+2: object",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# モデルの学習\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model = \u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmape_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ← カスタム評価関数を指定\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#early_stoppingあり\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/engine.py:297\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     booster = \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[32m    299\u001b[39m         booster.set_train_data_name(train_data_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/basic.py:3656\u001b[39m, in \u001b[36mBooster.__init__\u001b[39m\u001b[34m(self, params, train_set, model_file, model_str)\u001b[39m\n\u001b[32m   3649\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_network(\n\u001b[32m   3650\u001b[39m         machines=machines,\n\u001b[32m   3651\u001b[39m         local_listen_port=params[\u001b[33m\"\u001b[39m\u001b[33mlocal_listen_port\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3652\u001b[39m         listen_time_out=params.get(\u001b[33m\"\u001b[39m\u001b[33mtime_out\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m120\u001b[39m),\n\u001b[32m   3653\u001b[39m         num_machines=params[\u001b[33m\"\u001b[39m\u001b[33mnum_machines\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   3654\u001b[39m     )\n\u001b[32m   3655\u001b[39m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3656\u001b[39m \u001b[43mtrain_set\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3657\u001b[39m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[32m   3658\u001b[39m params.update(train_set.get_params())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/basic.py:2590\u001b[39m, in \u001b[36mDataset.construct\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2585\u001b[39m             \u001b[38;5;28mself\u001b[39m._set_init_score_by_predictor(\n\u001b[32m   2586\u001b[39m                 predictor=\u001b[38;5;28mself\u001b[39m._predictor, data=\u001b[38;5;28mself\u001b[39m.data, used_indices=used_indices\n\u001b[32m   2587\u001b[39m             )\n\u001b[32m   2588\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2590\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2597\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.free_raw_data:\n\u001b[32m   2604\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/basic.py:2123\u001b[39m, in \u001b[36mDataset._lazy_init\u001b[39m\u001b[34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[39m\n\u001b[32m   2121\u001b[39m     categorical_feature = reference.categorical_feature\n\u001b[32m   2122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[32m-> \u001b[39m\u001b[32m2123\u001b[39m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m.pandas_categorical = \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2130\u001b[39m     feature_name = data.column_names\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/basic.py:868\u001b[39m, in \u001b[36m_data_from_pandas\u001b[39m\u001b[34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[39m\n\u001b[32m    864\u001b[39m df_dtypes.append(np.float32)\n\u001b[32m    865\u001b[39m target_dtype = np.result_type(*df_dtypes)\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     \u001b[43m_pandas_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    869\u001b[39m     feature_name,\n\u001b[32m    870\u001b[39m     categorical_feature,\n\u001b[32m    871\u001b[39m     pandas_categorical,\n\u001b[32m    872\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/basic.py:814\u001b[39m, in \u001b[36m_pandas_to_numpy\u001b[39m\u001b[34m(data, target_dtype)\u001b[39m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pandas_to_numpy\u001b[39m(\n\u001b[32m    811\u001b[39m     data: pd_DataFrame,\n\u001b[32m    812\u001b[39m     target_dtype: \u001b[33m\"\u001b[39m\u001b[33mnp.typing.DTypeLike\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    813\u001b[39m ) -> np.ndarray:\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[43m_check_for_bad_pandas_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    816\u001b[39m         \u001b[38;5;66;03m# most common case (no nullable dtypes)\u001b[39;00m\n\u001b[32m    817\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m data.to_numpy(dtype=target_dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/My_Python_project/venv/lib/python3.11/site-packages/lightgbm/basic.py:805\u001b[39m, in \u001b[36m_check_for_bad_pandas_dtypes\u001b[39m\u001b[34m(pandas_dtypes_series)\u001b[39m\n\u001b[32m    799\u001b[39m bad_pandas_dtypes = [\n\u001b[32m    800\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpandas_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    801\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m column_name, pandas_dtype \u001b[38;5;129;01min\u001b[39;00m pandas_dtypes_series.items()\n\u001b[32m    802\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_allowed_numpy_dtype(pandas_dtype.type)\n\u001b[32m    803\u001b[39m ]\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bad_pandas_dtypes:\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    806\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpandas dtypes must be int, float or bool.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFields with bad pandas dtypes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(bad_pandas_dtypes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    807\u001b[39m     )\n",
            "\u001b[31mValueError\u001b[39m: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: addr1+2: object"
          ]
        }
      ],
      "source": [
        "# モデルの学習\n",
        "model = lgb.train(params,\n",
        "                  lgb_train,\n",
        "                  valid_sets=[lgb_train, lgb_test],\n",
        "                  feval=mape_eval,  # ← カスタム評価関数を指定\n",
        "                  callbacks=[lgb.early_stopping(stopping_rounds=1000, verbose=False)\n",
        "                  ]) #early_stoppingあり\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 08.評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータで予測\n",
        "y_pred = model.predict(X_valid , num_iteration=model.best_iteration)\n",
        "\n",
        "# 対数変換を戻す\n",
        "y_pred = np.expm1(y_pred)\n",
        "y_valid = np.expm1(y_valid)\n",
        "\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_pred = np.where(y_pred < 6000000, y_pred * 1.5, y_pred)\n",
        "# y_valid = np.where(y_valid < 6000000, y_valid * 1.5, y_valid)\n",
        "\n",
        "\n",
        "yp = pd.DataFrame(y_pred,columns=[\"%\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 特徴量の重要度\n",
        "print(\"特徴量の重要度\")\n",
        "lgb.plot_importance(model, figsize=(8,4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 09.可視化（importance SHAP）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C5HVNvdJA7rn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import lightgbm as lgb\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # パラメータの探索範囲を指定\n",
        "# param_grid = {\n",
        "#     'num_leaves': [20, 30, 40],\n",
        "#     'learning_rate': [0.01, 0.1, 0.5],\n",
        "#     'max_depth': [5, 10]\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # グリッドサーチCV\n",
        "# gsearch = GridSearchCV(gbm, param_grid, cv=5) #cvは交差検証の回数\n",
        "\n",
        "# # データを学習\n",
        "# gsearch.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "\n",
        "\n",
        "\n",
        "# # 最適なパラメータとスコアを表示\n",
        "# print('Best parameters found by grid search are:', gsearch.best_params_)\n",
        "# print('Best score:', gsearch.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqnAa0aHUkSF"
      },
      "outputs": [],
      "source": [
        "# # パラメータの辞書を結合\n",
        "# best_params = {**params, **gsearch.best_params_}\n",
        "\n",
        "# # 最適パラメータでモデルを再学習\n",
        "# model = lgb.LGBMClassifier(**best_params)\n",
        "# model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtosh8APQ1hX",
        "outputId": "7c1f8614-7b4c-4086-a173-77c653e5dee1"
      },
      "outputs": [],
      "source": [
        "#SHAP値の取得\n",
        "explainer = shap.TreeExplainer(model=model)#SHAP値を取得するためのモデル作成\n",
        "shap_values = explainer.shap_values(X=X_valid)#説明変数それぞれの値のSHAP値を取得する\n",
        "\n",
        "# print(shap_values)\n",
        "# print(shap_values.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aHCG_cntQ60c",
        "outputId": "0199ed8f-3e8f-4c5d-fe94-c5cc3c6bfade"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 変数別の影響度の可視化\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_valid)\n",
        "\n",
        "# 0番目のデータポイントを再選択\n",
        "i = 0\n",
        "single_observation = X_valid.iloc[i:i+1,:]\n",
        "\n",
        "#print(single_observation)\n",
        "\n",
        "\n",
        "# Explainerを使って説明を再計算\n",
        "single_shap_values = explainer(single_observation)\n",
        "\n",
        "# waterfallプロットの生成\n",
        "shap.waterfall_plot(single_shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.推論、提出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxCenyIHz0GT"
      },
      "source": [
        "## 提出用データの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G20PdnIGVCgo",
        "outputId": "49949858-f676-4cd2-d0d8-a37901c47b7b"
      },
      "outputs": [],
      "source": [
        "# 特徴量の選定\n",
        "df_test_p = df_test[feature_list]\n",
        "\n",
        "\n",
        "# 提出データに対する予測（確率値）\n",
        "y_scores_submit = model.predict(df_test_p)\n",
        "# # y_scores_binary_submit = np.where(y_scores_submit>0.5, 1, 0)\n",
        "\n",
        "\n",
        "y_scores_submit= np.expm1(y_scores_submit)\n",
        "\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_scores_submit = np.where(y_scores_submit < 6000000, y_scores_submit * 1.5, y_scores_submit)\n",
        "\n",
        "\n",
        "print(y_scores_submit)\n",
        "\n",
        "# y_scores_survive_submit = y_scores_submit[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lWNqV26CjYJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#提出用csvの作成\n",
        "df_scores_submit = pd.DataFrame(y_scores_submit)\n",
        "\n",
        "# df_submit = pd.concat([df_test[\"id\"], df_scores_submit], axis=1)\n",
        "# df_scores_submit.index = df_scores_submit.index + 1\n",
        "df_scores_submit.to_csv(\"submit.csv\", index=True, header=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAODvXF1D6XY",
        "outputId": "848cd83a-4da3-4cad-bb74-58f15e1af663"
      },
      "outputs": [],
      "source": [
        "# テストデータに対する予測（確率値）？？？？？？？？？？？？？？？？？？？？？？？・\n",
        "y_scores = model.predict(X_valid)\n",
        "# y_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZUHToQRPbBJ"
      },
      "source": [
        "## 誤差要因分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        },
        "id": "hE3uPKSQPdVh",
        "outputId": "74df9b47-512d-4730-c496-23a573297fad"
      },
      "outputs": [],
      "source": [
        "# valid予測\n",
        "y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "# 対数変換を戻す\n",
        "y_pred_valid = np.expm1(y_pred_valid)\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_pred_valid = np.where(y_pred_valid < 6000000, y_pred_valid * 1.5, y_pred_valid)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_eval = X_valid.copy()\n",
        "df_eval[\"y_true\"] = y_valid\n",
        "df_eval[\"y_pred\"] = y_pred_valid\n",
        "\n",
        "# APE計算（0割防止）\n",
        "df_eval[\"ape\"] = np.abs(df_eval[\"y_true\"] - df_eval[\"y_pred\"]) / np.maximum(df_eval[\"y_true\"], 1e-7)\n",
        "\n",
        "# 上位ワースト確認\n",
        "# df_eval.sort_values(\"ape\", ascending=False).head(20)\n",
        "df_eval[\"スコア差分\"] = df_eval[\"y_true\"] - df_eval[\"y_pred\"]\n",
        "df_eval[\"スコア差分\"].plot.hist(bins=50, figsize=(10,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVqE_gmXPgoo",
        "outputId": "d5a145cc-47ae-444d-9edc-23e8991b59fb"
      },
      "outputs": [],
      "source": [
        "# 価格帯ビン作成\n",
        "df_eval[\"price_bin\"] = pd.qcut(df_eval[\"y_true\"], q=5)\n",
        "\n",
        "# 価格帯別MAPE\n",
        "mape_by_bin = df_eval.groupby(\"price_bin\")[\"ape\"].mean() * 100\n",
        "print(\"価格帯別のMAPE\\n\", mape_by_bin)\n",
        "print()\n",
        "print(\"全データのMAPE\", df_eval[\"ape\"].mean())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "EPfb_aJJPmdK",
        "outputId": "f45737d4-06d4-40e4-d440-9b7bbde13b62"
      },
      "outputs": [],
      "source": [
        "def compare_distribution(col):\n",
        "    return pd.DataFrame({\n",
        "        \"train\": df_train[col].describe(),\n",
        "        \"valid\": X_valid[col].describe()\n",
        "    })\n",
        "\n",
        "compare_distribution(\"money_rimawari_now\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "65k3FdkrVQVN",
        "outputId": "7e011014-f0e4-4aa0-d436-02a474edaed2"
      },
      "outputs": [],
      "source": [
        "# APEが大きい上位10%\n",
        "threshold = df_eval[\"ape\"].quantile(0.9)\n",
        "bad_samples = df_eval[df_eval[\"ape\"] >= threshold]\n",
        "\n",
        "# SHAP値抽出\n",
        "shap_values_valid = explainer.shap_values(X_valid)\n",
        "shap_df = pd.DataFrame(\n",
        "    shap_values_valid,\n",
        "    columns=X_valid.columns,\n",
        "    index=X_valid.index\n",
        ")\n",
        "\n",
        "\n",
        "# 悪いサンプルのSHAP平均\n",
        "shap_df.loc[bad_samples.index].abs().mean().sort_values(ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GcuSM9FVYZK"
      },
      "outputs": [],
      "source": [
        "# shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values, features=X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUrvXmwxm9Qw",
        "outputId": "58581bdc-767f-41af-e598-d9c780d76025"
      },
      "outputs": [],
      "source": [
        "def calc_mape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-7))) * 100\n",
        "\n",
        "\n",
        "# valid 予測\n",
        "y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "# 対数変換を戻す\n",
        "y_pred_valid = np.expm1(y_pred_valid)\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_pred_valid = np.where(y_pred_valid < 6000000, y_pred_valid * 1.5, y_pred_valid)\n",
        "\n",
        "# MAPE 出力\n",
        "valid_mape = calc_mape(y_valid, y_pred_valid)\n",
        "print(f\"VALID MAPE: {valid_mape:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 対数変換を戻した y_pred_valid と y_valid はすでに存在すると仮定\n",
        "\n",
        "# X_valid[\"unite_area\"] = np.expm1(X_valid[\"unit_area\"])\n",
        "\n",
        "df_eval = X_valid.copy()\n",
        "df_eval[\"unit_area\"] = np.expm1(df_eval[\"unit_area\"])\n",
        "df_eval[\"y_true\"] = y_valid\n",
        "df_eval[\"y_pred\"] = y_pred_valid\n",
        "df_eval[\"ape\"] = np.abs(df_eval[\"y_true\"] - df_eval[\"y_pred\"]) / np.maximum(df_eval[\"y_true\"], 1e-7)\n",
        "\n",
        "# 上位10%の誤差サンプル抽出\n",
        "threshold = df_eval[\"ape\"].quantile(0.9)\n",
        "bad_samples = df_eval[df_eval[\"ape\"] >= threshold]\n",
        "\n",
        "# 調査対象の特徴量リスト\n",
        "check_features = [\n",
        "    \"unit_area\",\n",
        "    \"post1\",\n",
        "    \"floor_plan_code\",\n",
        "    \"walk_distance1\",\n",
        "    \"walk_distance2\",\n",
        "]\n",
        "\n",
        "# タグ系カラムを抽出（feature_list に基づく）\n",
        "tag_features = [col for col in feature_list if col.startswith(\"tag_\")]\n",
        "check_features.extend(tag_features)\n",
        "\n",
        "# 1. 数値特徴量の誤差 vs 値域の関係\n",
        "num_features = [\"unit_area\", \"walk_distance1\", \"walk_distance2\"]\n",
        "for col in num_features:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.scatterplot(data=df_eval, x=col, y=\"ape\", alpha=0.3)\n",
        "    sns.scatterplot(data=bad_samples, x=col, y=\"ape\", color=\"red\", alpha=0.5)\n",
        "    plt.title(f\"{col} と予測誤差(APE)の関係\")\n",
        "    plt.ylabel(\"APE\")\n",
        "    plt.xlabel(col)\n",
        "    plt.show()\n",
        "\n",
        "# 2. カテゴリ特徴量の誤差分布（箱ひげ図）\n",
        "cat_features = [\"post1\", \"floor_plan_code\"]\n",
        "for col in cat_features:\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.boxplot(x=col, y=\"ape\", data=df_eval)\n",
        "    plt.title(f\"{col} ごとの予測誤差(APE)分布\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# 3. タグ系特徴量の影響（悪いサンプルと全体比較）\n",
        "for col in tag_features:\n",
        "    if col not in df_eval.columns:\n",
        "        continue\n",
        "    mean_all = df_eval[col].mean()\n",
        "    mean_bad = bad_samples[col].mean()\n",
        "    print(f\"{col}: 全体平均={mean_all:.3f}, 誤差上位10%平均={mean_bad:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_eval[\"unit_area\"] = np.expm1(df_eval[\"unit_area\"])\n",
        "X_valid[\"unit_area\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
