{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aX0ZdLopCj-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GoogleColabにGitHubリポジトリをクローンする用\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "\n",
        "\n",
        "# !git clone https://github.com/keiseki-eng/My_Python_project"
      ],
      "metadata": {
        "id": "drmdF2ct3EBt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Zq_NLJ4i2H_f"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install ipykernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A8CzKrBbb7O",
        "outputId": "2bd12744-58c9-40f3-b16e-5387df304128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: japanize-matplotlib in /usr/local/lib/python3.12/dist-packages (1.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from japanize-matplotlib) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install japanize-matplotlib\n",
        "import os\n",
        "import sys\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "import japanize_matplotlib #日本語表示対応\n",
        "\n",
        "\n",
        "# Notebook から src ディレクトリを追加\n",
        "# sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
        "sys.path.append(\"/home/keiseki/My_Python_project/src\")\n",
        "\n",
        "# これで src/preprocess/make_tag_features.py が import 可能\n",
        "# from preprocess.make_tag_features import create_tag_features, extract_unique_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "csR9wtPUwBcd"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=pd.errors.PerformanceWarning\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vel6-57c2H_i"
      },
      "source": [
        "## 01.config読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "jOVigtnz2H_j"
      },
      "outputs": [],
      "source": [
        "# VSCode用\n",
        "# conf_path = os.path.join( '../config/config.yaml')\n",
        "# with open(conf_path, 'r') as f:\n",
        "#     config = yaml.safe_load(f)\n",
        "\n",
        "\n",
        "  # GoogleColab用\n",
        "conf_path = \"My_Python_project/config/config.yaml\"\n",
        "with open(conf_path, \"r\") as f:\n",
        "    config = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ugpvxdsV2H_k"
      },
      "outputs": [],
      "source": [
        "# 定義した特徴量リストを読み込み\n",
        "feature_list = config['FEATURE']['FEATURE_LIST']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "kH-U-6i52H_k"
      },
      "outputs": [],
      "source": [
        "# カテゴリカルカラムのリストを定義\n",
        "categorical_cols = config['FEATURE']['CATEGORICAL_COLS']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDwS_bGw2H_k"
      },
      "source": [
        "## 02.データ読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLIBSLbkZpre",
        "outputId": "ec1c3ac3-48cd-4508-cc27-5acf63d5a18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# ドライブ内のファイルパスを指定\n",
        "# train_path = '../20.Data/processed_train.pkl'\n",
        "# df_train = pd.read_pickle(train_path)\n",
        "\n",
        "\n",
        "# GoogleDriveをマウントしてファイル読み込み準備\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# ドライブ内のファイルパスを指定\n",
        "train_path = '/content/drive/MyDrive/Colab Notebooks/不動産予測/processed_train.pkl'\n",
        "df_train = pd.read_pickle(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "KfjIljDC2H_l",
        "outputId": "5dde17f0-b7b2-4fdc-cc63-838a27ee4a30"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../20.Data/processed_test.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3368527667.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# testデータの読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../20.Data/processed_test.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ドライブ内のファイルパスを指定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../20.Data/processed_test.pkl'"
          ]
        }
      ],
      "source": [
        "# testデータの読み込み\n",
        "# test_path = '../20.Data/processed_test.pkl'\n",
        "# df_test = pd.read_pickle(test_path)\n",
        "\n",
        "# ドライブ内のファイルパスを指定\n",
        "test_path = '/content/drive/MyDrive/Colab Notebooks/不動産予測/processed_test.pkl'\n",
        "df_test = pd.read_pickle(test_path)\n",
        "\n",
        "# df_test[\"renovation_date\"] = pd.to_datetime(df_test[\"renovation_date\"])\n",
        "# df_test[\"renovation_date\"] = df_test[\"renovation_date\"].astype(\"int64\") // 10**9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAg4x00Q2H_l"
      },
      "source": [
        "## 03.前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clrphVhW2H_l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # df_trainのカテゴリカルカラムを'category'型に変換\n",
        "# for col in categorical_cols:\n",
        "#     if col in df_train.columns:\n",
        "#         df_train[col] = df_train[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_EMDhiE2H_l"
      },
      "outputs": [],
      "source": [
        "# # df_testのカテゴリカルカラムを'category'型に変換\n",
        "# for col in categorical_cols:\n",
        "#     if col in df_test.columns:\n",
        "#         df_test[col] = df_test[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOa1ZBJx2H_l"
      },
      "outputs": [],
      "source": [
        "# # 極端に高い売買価格はクリッピング\n",
        "# lower = df_train[\"money_room\"].quantile(0.01)\n",
        "# upper = df_train[\"money_room\"].quantile(0.99)\n",
        "\n",
        "# df_train[\"money_room\"] = df_train[\"money_room\"].clip(lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsmjnEoQ2H_m"
      },
      "outputs": [],
      "source": [
        "# # 極端な値はクリッピング\n",
        "# for col in feature_list:\n",
        "#     # 数値型でなければスキップ\n",
        "#     if not pd.api.types.is_numeric_dtype(df_train[col]):\n",
        "#         continue\n",
        "\n",
        "#     # 全部NaNの列はスキップ\n",
        "#     if df_train[col].notna().sum() == 0:\n",
        "#         continue\n",
        "\n",
        "#     # 1% / 99% パーセンタイル\n",
        "#     lower = df_train[col].quantile(0.01)\n",
        "#     upper = df_train[col].quantile(0.99)\n",
        "\n",
        "#     # パーセンタイルが計算できない場合はスキップ\n",
        "#     if pd.isna(lower) or pd.isna(upper):\n",
        "#         continue\n",
        "\n",
        "#     # クリッピング\n",
        "#     df_train[col] = df_train[col].clip(lower, upper)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L8Cu6g82H_m"
      },
      "outputs": [],
      "source": [
        "# df_train[\"land_youto\"] = df_train[\"land_youto\"].where(\n",
        "#     df_train[\"land_youto\"].isin([8, 14]),\n",
        "#     np.nan\n",
        "# ).astype(\"category\")\n",
        "\n",
        "# df_test[\"land_youto\"] = df_test[\"land_youto\"].where(\n",
        "#     df_test[\"land_youto\"].isin([8, 14]),\n",
        "#     np.nan\n",
        "# ).astype(\"category\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPtoPAzO2H_m"
      },
      "outputs": [],
      "source": [
        "# df_train[\"building_type\"] = (\n",
        "#     df_train[\"building_type\"]\n",
        "#     .where(df_train[\"building_type\"].isin([1, 3]), np.nan)\n",
        "# )\n",
        "\n",
        "# df_test[\"building_type\"] = (\n",
        "#     df_test[\"building_type\"]\n",
        "#     .where(df_test[\"building_type\"].isin([1, 3]), np.nan)\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA7lhRak2H_m"
      },
      "outputs": [],
      "source": [
        "# # 欠損値はカラムの型に応じて補完\n",
        "# for col in df_train.columns:\n",
        "#     if col == \"money_room\": # 目的変数はこの時点では補完しない\n",
        "#         continue\n",
        "\n",
        "#     # if col in categorical_cols: # 明示的に定義されたカテゴリカルカラム\n",
        "#     #     # LightGBMが0をカテゴリとして扱えるため、0で補完\n",
        "#     #     df_train[col] = df_train[col].fillna(0)\n",
        "#     elif df_train[col].dtype in ['int64', 'float64']: # 数値カラム\n",
        "#         # 数値カラムは訓練データの中央値で補完\n",
        "#         median_val = df_train[col].median()\n",
        "#         df_train[col] = df_train[col].fillna(median_val)\n",
        "#         df_test[col] = df_test[col].fillna(median_val)\n",
        "#     # else:\n",
        "#         # # その他の型（objectなど）は、とりあえず0で補完（必要に応じて調整）\n",
        "#         # df_train[col] = df_train[col].fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPgMa-uR2H_m"
      },
      "outputs": [],
      "source": [
        "# # # room_count\n",
        "# # # unit_area\n",
        "# df_train = df_train[(df_train[\"unit_area\"]>12.5) & (df_train[\"unit_area\"]<300)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwC6x1832H_m"
      },
      "source": [
        "<!-- ##### 日付関係のデータは経過日数を特徴量とする -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUHhT2p12H_m"
      },
      "outputs": [],
      "source": [
        "# # 'target_ym'をdatetime型に変換 (月の最初の日と仮定)\n",
        "# df_train['target_date'] = pd.to_datetime(df_train['target_ym'].astype(str) + '01', format='%Y%m%d')\n",
        "# df_test['target_date'] = pd.to_datetime(df_test['target_ym'].astype(str) + '01', format='%Y%m%d')\n",
        "\n",
        "# # 'building_create_date'をdatetime型に変換\n",
        "# df_train['building_create_date'] = pd.to_datetime(df_train['building_create_date'])\n",
        "# df_test['building_create_date'] = pd.to_datetime(df_test['building_create_date'])\n",
        "# # 築年数を計算 (年単位)\n",
        "# df_train['building_age'] = (df_train['target_date'] - df_train['building_create_date']).dt.days // 365\n",
        "# df_test['building_age'] = (df_test['target_date'] - df_test['building_create_date']).dt.days // 365\n",
        "\n",
        "# # 'renovation_date'がUNIXタイムスタンプになっているため、再度datetimeに変換して経過年数を計算\n",
        "# # 既に'renovation_date'はUNIXタイムスタンプに変換済みのため、元の日付情報に戻すか、元のカラムを使用する\n",
        "# # ここでは、元のカラムが保持されていると仮定し、新しいカラムとして計算\n",
        "# # 注: もしdf_train['renovation_date']がUNIXタイムスタンプのみになっている場合、この処理は調整が必要です。\n",
        "# # 既存のrenovation_date（UNIXタイムスタンプ）を無視し、元の文字列カラムが存在しないため、新たに変換する\n",
        "# # もし元の文字列カラムが失われている場合は、UNIXタイムスタンプをdatetimeに変換する\n",
        "# # 現状のNotebookの処理から判断すると、renovation_dateはUNIXタイムスタンプなので、それを基に経過年数を計算します。\n",
        "\n",
        "# # UNIXタイムスタンプからdatetimeへの変換\n",
        "# df_train['renovation_datetime'] = pd.to_datetime(df_train['renovation_date'], errors='coerce')\n",
        "# df_test['renovation_datetime'] = pd.to_datetime(df_test['renovation_date'], errors='coerce')\n",
        "\n",
        "# # リノベーションからの経過年数を計算 (年単位)\n",
        "# # リノベーション日がない場合はNaNとなり、NaNとtarget_dateの差もNaNになるため、後でfillnaで処理\n",
        "# df_train['years_since_renovation'] = (df_train['target_date'] - df_train['renovation_datetime']).dt.days // 365\n",
        "# df_test['years_since_renovation'] = (df_test['target_date'] - df_test['renovation_datetime']).dt.days // 365\n",
        "# # 欠損値の補完\n",
        "# # building_ageとyears_since_renovationの負の値（未来の日付）やNaNを処理\n",
        "# # 例: 負の値やNaNを0で補完する\n",
        "# df_train['building_age'] = df_train['building_age'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "# df_test['building_age'] = df_test['building_age'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "\n",
        "# df_train['years_since_renovation'] = df_train['years_since_renovation'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "# df_test['years_since_renovation'] = df_test['years_since_renovation'].apply(lambda x: max(0, x) if pd.notna(x) else 0)\n",
        "# # 新しい特徴量をfeature_listに追加\n",
        "# new_features = ['building_age', 'years_since_renovation']\n",
        "# feature_list.extend(new_features)\n",
        "\n",
        "# # feature_listから'target_date', 'building_create_date', 'renovation_datetime'を削除 (直接特徴量として使わないため)\n",
        "# # (元のbuilding_create_dateとrenovation_dateはUNIX timestampとして残しておく)\n",
        "# feature_list = [f for f in feature_list if f not in ['target_date', 'building_create_date', 'renovation_date', 'renovation_datetime']]\n",
        "\n",
        "# # 確認\n",
        "# # print(df_train[feature_list].head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6l0Z13Y2H_n"
      },
      "source": [
        "<!-- ## 04.特徴量生成 -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWjvgKaS2H_n"
      },
      "outputs": [],
      "source": [
        "# # タグIDの分割により特徴量を追加\n",
        "\n",
        "# # all_unique_tags という空の set を初期化\n",
        "# all_unique_tags = set()\n",
        "# # df_train の 'statuses' カラムに対して extract_unique_tags を呼び出し、得られたユニークタグを追加\n",
        "# all_unique_tags.update(extract_unique_tags(df_train, 'statuses'))\n",
        "# # df_test の 'statuses' カラムに対して extract_unique_tags を呼び出し、得られたユニークタグを追加\n",
        "# all_unique_tags.update(extract_unique_tags(df_test, 'statuses'))\n",
        "# # all_unique_tags をソートしてリストに変換し、all_unique_tags_list に格納\n",
        "# all_unique_tags_list = sorted(list(all_unique_tags))\n",
        "\n",
        "# # create_tag_features 関数を df_train と df_test に適用\n",
        "# df_train, new_tag_features = create_tag_features(df_train.copy(), all_unique_tags_list)\n",
        "# df_test, _ = create_tag_features(df_test.copy(), all_unique_tags_list)\n",
        "# # 既存の feature_list に new_tag_features を追加\n",
        "# feature_list.extend(new_tag_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VNwfH3v2H_n"
      },
      "outputs": [],
      "source": [
        "# # LightGBM 用に使用するカテゴリ特徴量のリストを作成\n",
        "# categorical_features = [\n",
        "#     col for col in categorical_cols\n",
        "#     if col in feature_list\n",
        "# ]\n",
        "\n",
        "# feature_list = feature_list.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNkth9VW2H_n"
      },
      "source": [
        "## 05.train/valid 分割　＆　target加工"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDYt7poo2H_n"
      },
      "outputs": [],
      "source": [
        "# X_all, X_train, X_validの再構築\n",
        "X_all = df_train[feature_list]\n",
        "y_all = df_train[\"money_room\"]\n",
        "\n",
        "# log変換前の元価格を保存（後段の重み付け用）\n",
        "y_price_raw = df_train.loc[X_all.index, \"money_room\"]\n",
        "\n",
        "# unit_areaも対数変換\n",
        "X_all[\"unit_area\"] = np.log1p(X_all[\"unit_area\"])\n",
        "\n",
        "# 目的変数が右に裾野が広いので対数変換\n",
        "y_all = np.log1p(y_all)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5lPFsjA2H_o"
      },
      "source": [
        "## 06.sample_weight適用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzGe7M7I2H_o"
      },
      "outputs": [],
      "source": [
        "# 元スケールの価格\n",
        "y_price = y_all\n",
        "\n",
        "\n",
        "# train / valid に合わせる\n",
        "y_price_raw_train = y_price_raw.loc[X_train.index]\n",
        "\n",
        "# 低価格ほど重く（価格の逆数）\n",
        "sample_weight = 1 / np.log1p(np.maximum(y_price_raw_train, 1_000_000)* np.where(y_price_raw_train < 13_000_000, 2.0, 1.0) )\n",
        "\n",
        "# 正規化\n",
        "sample_weight = sample_weight / sample_weight.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orbb7AzF2H_o"
      },
      "source": [
        "## 07.モデル学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9UzfPSES7je"
      },
      "outputs": [],
      "source": [
        "# カスタム評価関数（eval_metric形式）\n",
        "def mape_eval(preds, train_data):\n",
        "    y_true = np.expm1(train_data.get_label())\n",
        "    y_pred = np.expm1(preds)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-7))) * 100\n",
        "    return 'mape', mape, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UthK6de1T6Ty"
      },
      "outputs": [],
      "source": [
        "# LightGBM のパラメータ設定\n",
        "params = config['MODEL_PARAMS']\n",
        "\n",
        "# LightGBM のデータセットを作成\n",
        "lgb_train = lgb.Dataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    weight=sample_weight.loc[X_train.index],\n",
        "    categorical_feature=categorical_cols\n",
        ")\n",
        "\n",
        "lgb_test = lgb.Dataset(\n",
        "    X_valid,\n",
        "    y_valid,\n",
        "    reference=lgb_train,\n",
        "    categorical_feature=categorical_cols\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V4dTzGRyA2XQ"
      },
      "outputs": [],
      "source": [
        "# モデルの学習\n",
        "model = lgb.train(params,\n",
        "                  lgb_train,\n",
        "                  valid_sets=[lgb_train, lgb_test],\n",
        "                  feval=mape_eval,  # ← カスタム評価関数を指定\n",
        "                  callbacks=[lgb.early_stopping(stopping_rounds=1000, verbose=False)\n",
        "                  ]) #early_stoppingあり\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ_AzEqc2H_p"
      },
      "source": [
        "## 08.評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ8MsZmS2H_p"
      },
      "outputs": [],
      "source": [
        "# テストデータで予測\n",
        "y_pred = model.predict(X_valid , num_iteration=model.best_iteration)\n",
        "\n",
        "# 対数変換を戻す\n",
        "y_pred = np.expm1(y_pred)\n",
        "y_valid = np.expm1(y_valid)\n",
        "\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_pred = np.where(y_pred < 6000000, y_pred * 1.5, y_pred)\n",
        "# y_valid = np.where(y_valid < 6000000, y_valid * 1.5, y_valid)\n",
        "\n",
        "\n",
        "yp = pd.DataFrame(y_pred,columns=[\"%\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 特徴量の重要度\n",
        "print(\"特徴量の重要度\")\n",
        "lgb.plot_importance(model, figsize=(8,4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR1rdmTd2H_p"
      },
      "source": [
        "## 09.可視化（importance SHAP）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C5HVNvdJA7rn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import lightgbm as lgb\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # パラメータの探索範囲を指定\n",
        "# param_grid = {\n",
        "#     'num_leaves': [20, 30, 40],\n",
        "#     'learning_rate': [0.01, 0.1, 0.5],\n",
        "#     'max_depth': [5, 10]\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # グリッドサーチCV\n",
        "# gsearch = GridSearchCV(gbm, param_grid, cv=5) #cvは交差検証の回数\n",
        "\n",
        "# # データを学習\n",
        "# gsearch.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)])\n",
        "\n",
        "\n",
        "\n",
        "# # 最適なパラメータとスコアを表示\n",
        "# print('Best parameters found by grid search are:', gsearch.best_params_)\n",
        "# print('Best score:', gsearch.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqnAa0aHUkSF"
      },
      "outputs": [],
      "source": [
        "# # パラメータの辞書を結合\n",
        "# best_params = {**params, **gsearch.best_params_}\n",
        "\n",
        "# # 最適パラメータでモデルを再学習\n",
        "# model = lgb.LGBMClassifier(**best_params)\n",
        "# model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtosh8APQ1hX"
      },
      "outputs": [],
      "source": [
        "#SHAP値の取得\n",
        "explainer = shap.TreeExplainer(model=model)#SHAP値を取得するためのモデル作成\n",
        "shap_values = explainer.shap_values(X=X_valid)#説明変数それぞれの値のSHAP値を取得する\n",
        "\n",
        "# print(shap_values)\n",
        "# print(shap_values.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHCG_cntQ60c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 変数別の影響度の可視化\n",
        "shap.initjs()\n",
        "shap.summary_plot(shap_values, X_valid)\n",
        "\n",
        "# 0番目のデータポイントを再選択\n",
        "i = 0\n",
        "single_observation = X_valid.iloc[i:i+1,:]\n",
        "\n",
        "#print(single_observation)\n",
        "\n",
        "\n",
        "# Explainerを使って説明を再計算\n",
        "single_shap_values = explainer(single_observation)\n",
        "\n",
        "# waterfallプロットの生成\n",
        "shap.waterfall_plot(single_shap_values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQfAs_kZ2H_v"
      },
      "source": [
        "## 10.推論、提出"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxCenyIHz0GT"
      },
      "source": [
        "## 提出用データの作成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G20PdnIGVCgo"
      },
      "outputs": [],
      "source": [
        "# 特徴量の選定\n",
        "df_test_p = df_test[feature_list]\n",
        "\n",
        "\n",
        "# 提出データに対する予測（確率値）\n",
        "y_scores_submit = model.predict(df_test_p)\n",
        "# # y_scores_binary_submit = np.where(y_scores_submit>0.5, 1, 0)\n",
        "\n",
        "\n",
        "y_scores_submit= np.expm1(y_scores_submit)\n",
        "\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_scores_submit = np.where(y_scores_submit < 6000000, y_scores_submit * 1.5, y_scores_submit)\n",
        "\n",
        "\n",
        "print(y_scores_submit)\n",
        "\n",
        "# y_scores_survive_submit = y_scores_submit[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lWNqV26CjYJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#提出用csvの作成\n",
        "df_scores_submit = pd.DataFrame(y_scores_submit)\n",
        "\n",
        "# df_submit = pd.concat([df_test[\"id\"], df_scores_submit], axis=1)\n",
        "# df_scores_submit.index = df_scores_submit.index + 1\n",
        "df_scores_submit.to_csv(\"submit.csv\", index=True, header=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAODvXF1D6XY"
      },
      "outputs": [],
      "source": [
        "# テストデータに対する予測（確率値）？？？？？？？？？？？？？？？？？？？？？？？・\n",
        "y_scores = model.predict(X_valid)\n",
        "# y_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZUHToQRPbBJ"
      },
      "source": [
        "## 誤差要因分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE3uPKSQPdVh"
      },
      "outputs": [],
      "source": [
        "# valid予測\n",
        "y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "# 対数変換を戻す\n",
        "y_pred_valid = np.expm1(y_pred_valid)\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_pred_valid = np.where(y_pred_valid < 6000000, y_pred_valid * 1.5, y_pred_valid)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_eval = X_valid.copy()\n",
        "df_eval[\"y_true\"] = y_valid\n",
        "df_eval[\"y_pred\"] = y_pred_valid\n",
        "\n",
        "# APE計算（0割防止）\n",
        "df_eval[\"ape\"] = np.abs(df_eval[\"y_true\"] - df_eval[\"y_pred\"]) / np.maximum(df_eval[\"y_true\"], 1e-7)\n",
        "\n",
        "# 上位ワースト確認\n",
        "# df_eval.sort_values(\"ape\", ascending=False).head(20)\n",
        "df_eval[\"スコア差分\"] = df_eval[\"y_true\"] - df_eval[\"y_pred\"]\n",
        "df_eval[\"スコア差分\"].plot.hist(bins=50, figsize=(10,5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVqE_gmXPgoo"
      },
      "outputs": [],
      "source": [
        "# 価格帯ビン作成\n",
        "df_eval[\"price_bin\"] = pd.qcut(df_eval[\"y_true\"], q=5)\n",
        "\n",
        "# 価格帯別MAPE\n",
        "mape_by_bin = df_eval.groupby(\"price_bin\")[\"ape\"].mean() * 100\n",
        "print(\"価格帯別のMAPE\\n\", mape_by_bin)\n",
        "print()\n",
        "print(\"全データのMAPE\", df_eval[\"ape\"].mean())\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPfb_aJJPmdK"
      },
      "outputs": [],
      "source": [
        "def compare_distribution(col):\n",
        "    return pd.DataFrame({\n",
        "        \"train\": df_train[col].describe(),\n",
        "        \"valid\": X_valid[col].describe()\n",
        "    })\n",
        "\n",
        "# compare_distribution(\"money_rimawari_now\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65k3FdkrVQVN"
      },
      "outputs": [],
      "source": [
        "# APEが大きい上位10%\n",
        "threshold = df_eval[\"ape\"].quantile(0.9)\n",
        "bad_samples = df_eval[df_eval[\"ape\"] >= threshold]\n",
        "\n",
        "# SHAP値抽出\n",
        "shap_values_valid = explainer.shap_values(X_valid)\n",
        "shap_df = pd.DataFrame(\n",
        "    shap_values_valid,\n",
        "    columns=X_valid.columns,\n",
        "    index=X_valid.index\n",
        ")\n",
        "\n",
        "\n",
        "# 悪いサンプルのSHAP平均\n",
        "shap_df.loc[bad_samples.index].abs().mean().sort_values(ascending=False).head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GcuSM9FVYZK"
      },
      "outputs": [],
      "source": [
        "# shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values, features=X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUrvXmwxm9Qw"
      },
      "outputs": [],
      "source": [
        "def calc_mape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / np.maximum(y_true, 1e-7))) * 100\n",
        "\n",
        "\n",
        "# valid 予測\n",
        "y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
        "# 対数変換を戻す\n",
        "y_pred_valid = np.expm1(y_pred_valid)\n",
        "# ★小細工:予測値が低価格帯であれば、予測値を1.5倍にする\n",
        "# y_pred_valid = np.where(y_pred_valid < 6000000, y_pred_valid * 1.5, y_pred_valid)\n",
        "\n",
        "# MAPE 出力\n",
        "valid_mape = calc_mape(y_valid, y_pred_valid)\n",
        "print(f\"VALID MAPE: {valid_mape:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eicRmSRU2H_x"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 対数変換を戻した y_pred_valid と y_valid はすでに存在すると仮定\n",
        "\n",
        "# X_valid[\"unite_area\"] = np.expm1(X_valid[\"unit_area\"])\n",
        "\n",
        "df_eval = X_valid.copy()\n",
        "df_eval[\"unit_area\"] = np.expm1(df_eval[\"unit_area\"])\n",
        "df_eval[\"y_true\"] = y_valid\n",
        "df_eval[\"y_pred\"] = y_pred_valid\n",
        "df_eval[\"ape\"] = np.abs(df_eval[\"y_true\"] - df_eval[\"y_pred\"]) / np.maximum(df_eval[\"y_true\"], 1e-7)\n",
        "\n",
        "# 上位10%の誤差サンプル抽出\n",
        "threshold = df_eval[\"ape\"].quantile(0.9)\n",
        "bad_samples = df_eval[df_eval[\"ape\"] >= threshold]\n",
        "\n",
        "# 調査対象の特徴量リスト\n",
        "check_features = [\n",
        "    \"unit_area\",\n",
        "    \"post1\",\n",
        "    \"floor_plan_code\",\n",
        "    \"walk_distance1\",\n",
        "    \"walk_distance2\",\n",
        "]\n",
        "\n",
        "# タグ系カラムを抽出（feature_list に基づく）\n",
        "tag_features = [col for col in feature_list if col.startswith(\"tag_\")]\n",
        "check_features.extend(tag_features)\n",
        "\n",
        "# 1. 数値特徴量の誤差 vs 値域の関係\n",
        "num_features = [\"unit_area\", \"walk_distance1\", \"walk_distance2\"]\n",
        "for col in num_features:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.scatterplot(data=df_eval, x=col, y=\"ape\", alpha=0.3)\n",
        "    sns.scatterplot(data=bad_samples, x=col, y=\"ape\", color=\"red\", alpha=0.5)\n",
        "    plt.title(f\"{col} と予測誤差(APE)の関係\")\n",
        "    plt.ylabel(\"APE\")\n",
        "    plt.xlabel(col)\n",
        "    plt.show()\n",
        "\n",
        "# 2. カテゴリ特徴量の誤差分布（箱ひげ図）\n",
        "cat_features = [\"post1\", \"floor_plan_code\"]\n",
        "for col in cat_features:\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.boxplot(x=col, y=\"ape\", data=df_eval)\n",
        "    plt.title(f\"{col} ごとの予測誤差(APE)分布\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# 3. タグ系特徴量の影響（悪いサンプルと全体比較）\n",
        "for col in tag_features:\n",
        "    if col not in df_eval.columns:\n",
        "        continue\n",
        "    mean_all = df_eval[col].mean()\n",
        "    mean_bad = bad_samples[col].mean()\n",
        "    print(f\"{col}: 全体平均={mean_all:.3f}, 誤差上位10%平均={mean_bad:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3nVh5UZ2H_y"
      },
      "outputs": [],
      "source": [
        "df_eval[\"unit_area\"] = np.expm1(df_eval[\"unit_area\"])\n",
        "X_valid[\"unit_area\"]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}